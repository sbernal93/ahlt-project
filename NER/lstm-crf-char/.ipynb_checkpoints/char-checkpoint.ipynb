{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Conv1D, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def instances(fi):\n",
    "    xseq = []\n",
    "    yseq = []\n",
    "\n",
    "    for line in fi:\n",
    "        line = line.strip('\\n')\n",
    "        if not line:\n",
    "            # An empty line means the end of a sentence.\n",
    "            # Return accumulated sequences, and reinitialize.\n",
    "            yield xseq, yseq\n",
    "            xseq = []\n",
    "            yseq = []\n",
    "            continue\n",
    "\n",
    "        # Split the line with TAB characters.\n",
    "        fields = line.split('\\t')\n",
    "\n",
    "        # Append the item features to the item sequence.\n",
    "        # fields are:  0=sid, 1=form, 2=span_start, 3=span_end, 4=tag, 5...N = features\n",
    "        item = fields[5:]\n",
    "        xseq.append(item)\n",
    "\n",
    "        # Append the label to the label sequence.\n",
    "        yseq.append(fields[4])\n",
    "    #return xseq, yseq\n",
    "\n",
    "def load_data(fi):\n",
    "    xtrain = []\n",
    "    ytrain = []\n",
    "\n",
    "    for line in fi:\n",
    "        line = line.strip('\\n')\n",
    "        if not line:\n",
    "            continue\n",
    "        # Split the line with TAB characters.\n",
    "        fields = line.split('\\t')\n",
    "\n",
    "        # Append the item features to the item sequence.\n",
    "        # fields are:  0=sid, 1=form, 2=span_start, 3=span_end, 4=tag, 5...N = features\n",
    "        item = fields[5:]\n",
    "        xtrain.append(item)\n",
    "\n",
    "        # Append the label to the label sequence.\n",
    "        ytrain.append(fields[4])\n",
    "    return xtrain, ytrain\n",
    "\n",
    "def instances_pred(fi):\n",
    "    xseq = []\n",
    "    toks = []\n",
    "\n",
    "    for line in fi:\n",
    "        line = line.strip('\\n')\n",
    "        if not line:\n",
    "            # An empty line means the end of a sentence.\n",
    "            # Return accumulated sequences, and reinitialize.\n",
    "            yield xseq, toks\n",
    "            xseq = []\n",
    "            toks = []\n",
    "            continue\n",
    "\n",
    "        # Split the line with TAB characters.\n",
    "        fields = line.split('\\t')\n",
    "\n",
    "        # Append the item features to the item sequence.\n",
    "        # fields are:  0=sid, 1=form, 2=span_start, 3=span_end, 4=tag, 5...N = features\n",
    "        item = fields[5:]\n",
    "        xseq.append(item)\n",
    "\n",
    "        # Append the label to the label sequence.\n",
    "        toks.append([fields[0],fields[1],fields[2],fields[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tags = []\n",
    "\n",
    "fi = open('../lstm-crf-glove/results/1/train.cod')\n",
    "\n",
    "# Read sentences from STDIN, and append them to the trainer.\n",
    "for xseq, yseq in instances(fi):\n",
    "    sentences.append(xseq)\n",
    "    tags.append(yseq)\n",
    "fi.close()\n",
    "fi = open('../lstm-crf-glove/results/1/train.cod')\n",
    "(xtrain, ytrain) = load_data(fi)\n",
    "x = pd.DataFrame(xtrain)\n",
    "y = pd.DataFrame(ytrain)[0].values\n",
    "\n",
    "max_len = 75\n",
    "words = list(set(x[0].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "utags = list(set(y))\n",
    "n_tags = len(utags)\n",
    "\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "tag2idx = {t: i for i, t in enumerate(utags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "\n",
    "docs = [[w[0] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, value=word2idx[\"PAD\"], padding='post', truncating='post')\n",
    "\n",
    "Y = [[tag2idx[t] for t in tag ]for tag in tags]\n",
    "Y = pad_sequences(maxlen=max_len, sequences=Y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n",
    "#Y = [to_categorical(i, num_classes=n_tags) for i in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "max_len_char = 10\n",
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)\n",
    "\n",
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"PAD\"] = 0\n",
    "\n",
    "X_char = []\n",
    "for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 2, output_dim=20,\n",
    "             input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=10,\n",
    "                   input_length=max_len_char, mask_zero=True))(char_in)\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=20, return_sequences=False,\n",
    "                        recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                       recurrent_dropout=0.6))(x)\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_lstm)\n",
    "#crf = CRF(n_tags)  # CRF layer\n",
    "#out = crf(model)  # output\n",
    "\n",
    "model = Model([word_in, char_in], out)\n",
    "#model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 75, 10, 10)   820         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 75, 20)       161100      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_23 (TimeDistri (None, 75, 20)       2480        time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 75, 40)       0           embedding_15[0][0]               \n",
      "                                                                 time_distributed_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 75, 40)       0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 75, 100)      36400       spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_24 (TimeDistri (None, 75, 10)       1010        bidirectional_8[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 201,810\n",
      "Trainable params: 201,810\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_30 to have shape (75, 1) but got array with shape (75, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-a77cc70cbe09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit([X, np.array(X_char).reshape((len(X_char), max_len, max_len_char))],\n\u001b[1;32m      2\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 validation_split=0.1, verbose=1)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_30 to have shape (75, 1) but got array with shape (75, 9)"
     ]
    }
   ],
   "source": [
    "history = model.fit([X, np.array(X_char).reshape((len(X_char), max_len, max_len_char))],\n",
    "                    np.array(Y), batch_size=32, epochs=5,\n",
    "                validation_split=0.1, verbose=1)\n",
    "\n",
    "hist = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xseq,toks in instances_pred(open('../lstm-crf-glove/results/2/test.cod')):\n",
    "    #x = pd.DataFrame(xseq)\n",
    "    x_test_sent = [[word2idx.get(t[0],0) for t in xseq]]\n",
    "    x_test_sent = pad_sequences(maxlen=max_len, sequences=x_test_sent, padding=\"post\", value=n_words-1)\n",
    "    \n",
    "    sentence = [t[0] for t in xseq]\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "\n",
    "    #prediction = model.predict(np.array([x_test_sent[0]]))[0]\n",
    "    #model.predict([X_word_te,\n",
    "    #                    np.array(X_char_te).reshape((len(X_char_te),\n",
    "    #                                                 max_len, max_len_char))])\n",
    "    x_test_sent = np.array([x_test_sent[0]])\n",
    "    prediction = model.predict([x_test_sent, np.array(sent_seq).reshape((len(x_test_sent), max_len, max_len_char))])[0]\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    inside = False;\n",
    "    for k in range(0,len(toks)) :\n",
    "        y = utags[prediction[k]]\n",
    "        (sid, form, offS, offE) = toks[k]\n",
    "\n",
    "        if (y[0]==\"B\") :\n",
    "            entity_form = form\n",
    "            entity_start = offS\n",
    "            entity_end = offE\n",
    "            entity_type = y[2:]\n",
    "            inside = True\n",
    "        elif (y[0]==\"I\" and inside) :\n",
    "            entity_form += \" \"+form\n",
    "            entity_end = offE\n",
    "        elif (y[0]==\"O\" and inside) :\n",
    "            #print(sid, entity_start+\"-\"+entity_end, entity_form, entity_type, sep=\"|\")\n",
    "            out.write(sid + \"|\" + entity_start+\"-\"+entity_end + \"|\" +entity_form + \"|\" +entity_type + \"\\n\")\n",
    "            inside = False\n",
    "\n",
    "    if inside : out.write(sid + \"|\" + entity_start+\"-\"+entity_end + \"|\" + entity_form + \"|\" + entity_type + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'that',\n",
       " 'concomitant',\n",
       " 'use',\n",
       " 'of',\n",
       " 'other',\n",
       " 'known',\n",
       " 'photosensitizing',\n",
       " 'agents',\n",
       " 'might',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'photosensitivity',\n",
       " 'reaction',\n",
       " 'of',\n",
       " 'actinic',\n",
       " 'keratoses',\n",
       " 'treated',\n",
       " 'with',\n",
       " 'methyl',\n",
       " 'aminolevulinate',\n",
       " 'cream',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
